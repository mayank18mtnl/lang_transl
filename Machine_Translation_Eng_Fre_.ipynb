{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "august-commercial",
      "metadata": {
        "id": "august-commercial"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "breathing-frequency",
      "metadata": {
        "id": "breathing-frequency"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "epochs = 100\n",
        "latent_dim = 256\n",
        "num_samples = 20000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "provincial-religious",
      "metadata": {
        "id": "provincial-religious"
      },
      "outputs": [],
      "source": [
        "data_path = '/content/sample_data/fra.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "piano-martial",
      "metadata": {
        "id": "piano-martial"
      },
      "outputs": [],
      "source": [
        "input_texts = []\n",
        "output_texts = []\n",
        "input_characters = set()\n",
        "output_characters = set()\n",
        "\n",
        "with open(data_path, 'r', encoding = 'utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "for line in lines[:min(num_samples,len(lines)-1)]:\n",
        "    input_text, output_text, _ = line.split('\\t')\n",
        "\n",
        "    # We use 'tab' as the 'start sequence' character\n",
        "    # for the targets, and '\\n' as the 'end sequence' character.\n",
        "\n",
        "    output_text = '\\t' + output_text + '\\n'\n",
        "    input_texts.append(input_text)\n",
        "    output_texts.append(output_text)\n",
        "\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in output_text:\n",
        "        if char not in output_characters:\n",
        "            output_characters.add(char)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Z-5iwqm-lrNP"
      },
      "id": "Z-5iwqm-lrNP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "attractive-biology",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "attractive-biology",
        "outputId": "2412444c-d4d5-4324-edd8-824420a45ff7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "101"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "len(output_characters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "earlier-sixth",
      "metadata": {
        "id": "earlier-sixth"
      },
      "outputs": [],
      "source": [
        "input_characters = sorted(list(input_characters))\n",
        "output_characters = sorted(list(output_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(output_characters)\n",
        "max_encoder_seq_length = max([len(text) for text in input_texts])\n",
        "max_decoder_seq_length = max([len(text) for text in output_texts])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "greater-tablet",
      "metadata": {
        "id": "greater-tablet",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be3be257-631d-433e-a293-7948b87c4469"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Samples: 20000\n",
            "Number of unique input Tokens: 74\n",
            "Number of unique output Tokens: 101\n",
            "Max sequence length for inputs: 17\n",
            "Max sequence length for outputs: 59\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of Samples:\", len(input_texts))\n",
        "print('Number of unique input Tokens:' , num_encoder_tokens)\n",
        "print('Number of unique output Tokens:' , num_decoder_tokens)\n",
        "print('Max sequence length for inputs:' , max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:' , max_decoder_seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hawaiian-acrylic",
      "metadata": {
        "id": "hawaiian-acrylic"
      },
      "outputs": [],
      "source": [
        "input_token_index = dict(\n",
        "    [(char,i) for i,char in enumerate(input_characters)])\n",
        "\n",
        "output_token_index = dict(\n",
        "    [(char,i) for i,char in enumerate(output_characters)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "comic-bunch",
      "metadata": {
        "id": "comic-bunch",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58bc16fe-492f-4b74-c59f-bc9c6044943c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({' ': 0,\n",
              "  '!': 1,\n",
              "  '\"': 2,\n",
              "  '$': 3,\n",
              "  '%': 4,\n",
              "  '&': 5,\n",
              "  \"'\": 6,\n",
              "  ',': 7,\n",
              "  '-': 8,\n",
              "  '.': 9,\n",
              "  '/': 10,\n",
              "  '0': 11,\n",
              "  '1': 12,\n",
              "  '2': 13,\n",
              "  '3': 14,\n",
              "  '4': 15,\n",
              "  '5': 16,\n",
              "  '6': 17,\n",
              "  '7': 18,\n",
              "  '8': 19,\n",
              "  '9': 20,\n",
              "  ':': 21,\n",
              "  '?': 22,\n",
              "  'A': 23,\n",
              "  'B': 24,\n",
              "  'C': 25,\n",
              "  'D': 26,\n",
              "  'E': 27,\n",
              "  'F': 28,\n",
              "  'G': 29,\n",
              "  'H': 30,\n",
              "  'I': 31,\n",
              "  'J': 32,\n",
              "  'K': 33,\n",
              "  'L': 34,\n",
              "  'M': 35,\n",
              "  'N': 36,\n",
              "  'O': 37,\n",
              "  'P': 38,\n",
              "  'Q': 39,\n",
              "  'R': 40,\n",
              "  'S': 41,\n",
              "  'T': 42,\n",
              "  'U': 43,\n",
              "  'V': 44,\n",
              "  'W': 45,\n",
              "  'Y': 46,\n",
              "  'a': 47,\n",
              "  'b': 48,\n",
              "  'c': 49,\n",
              "  'd': 50,\n",
              "  'e': 51,\n",
              "  'f': 52,\n",
              "  'g': 53,\n",
              "  'h': 54,\n",
              "  'i': 55,\n",
              "  'j': 56,\n",
              "  'k': 57,\n",
              "  'l': 58,\n",
              "  'm': 59,\n",
              "  'n': 60,\n",
              "  'o': 61,\n",
              "  'p': 62,\n",
              "  'q': 63,\n",
              "  'r': 64,\n",
              "  's': 65,\n",
              "  't': 66,\n",
              "  'u': 67,\n",
              "  'v': 68,\n",
              "  'w': 69,\n",
              "  'x': 70,\n",
              "  'y': 71,\n",
              "  'z': 72,\n",
              "  'é': 73},\n",
              " {'\\t': 0,\n",
              "  '\\n': 1,\n",
              "  ' ': 2,\n",
              "  '!': 3,\n",
              "  '$': 4,\n",
              "  '%': 5,\n",
              "  '&': 6,\n",
              "  \"'\": 7,\n",
              "  '(': 8,\n",
              "  ')': 9,\n",
              "  ',': 10,\n",
              "  '-': 11,\n",
              "  '.': 12,\n",
              "  '0': 13,\n",
              "  '1': 14,\n",
              "  '2': 15,\n",
              "  '3': 16,\n",
              "  '4': 17,\n",
              "  '5': 18,\n",
              "  '6': 19,\n",
              "  '7': 20,\n",
              "  '8': 21,\n",
              "  '9': 22,\n",
              "  ':': 23,\n",
              "  '?': 24,\n",
              "  'A': 25,\n",
              "  'B': 26,\n",
              "  'C': 27,\n",
              "  'D': 28,\n",
              "  'E': 29,\n",
              "  'F': 30,\n",
              "  'G': 31,\n",
              "  'H': 32,\n",
              "  'I': 33,\n",
              "  'J': 34,\n",
              "  'K': 35,\n",
              "  'L': 36,\n",
              "  'M': 37,\n",
              "  'N': 38,\n",
              "  'O': 39,\n",
              "  'P': 40,\n",
              "  'Q': 41,\n",
              "  'R': 42,\n",
              "  'S': 43,\n",
              "  'T': 44,\n",
              "  'U': 45,\n",
              "  'V': 46,\n",
              "  'W': 47,\n",
              "  'Y': 48,\n",
              "  'a': 49,\n",
              "  'b': 50,\n",
              "  'c': 51,\n",
              "  'd': 52,\n",
              "  'e': 53,\n",
              "  'f': 54,\n",
              "  'g': 55,\n",
              "  'h': 56,\n",
              "  'i': 57,\n",
              "  'j': 58,\n",
              "  'k': 59,\n",
              "  'l': 60,\n",
              "  'm': 61,\n",
              "  'n': 62,\n",
              "  'o': 63,\n",
              "  'p': 64,\n",
              "  'q': 65,\n",
              "  'r': 66,\n",
              "  's': 67,\n",
              "  't': 68,\n",
              "  'u': 69,\n",
              "  'v': 70,\n",
              "  'w': 71,\n",
              "  'x': 72,\n",
              "  'y': 73,\n",
              "  'z': 74,\n",
              "  '\\xa0': 75,\n",
              "  '«': 76,\n",
              "  '»': 77,\n",
              "  'À': 78,\n",
              "  'Ç': 79,\n",
              "  'É': 80,\n",
              "  'Ê': 81,\n",
              "  'Ô': 82,\n",
              "  'à': 83,\n",
              "  'â': 84,\n",
              "  'ç': 85,\n",
              "  'è': 86,\n",
              "  'é': 87,\n",
              "  'ê': 88,\n",
              "  'ë': 89,\n",
              "  'î': 90,\n",
              "  'ï': 91,\n",
              "  'ô': 92,\n",
              "  'ù': 93,\n",
              "  'û': 94,\n",
              "  'œ': 95,\n",
              "  '\\u2009': 96,\n",
              "  '‘': 97,\n",
              "  '’': 98,\n",
              "  '\\u202f': 99,\n",
              "  '‽': 100})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "input_token_index , output_token_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deluxe-finnish",
      "metadata": {
        "id": "deluxe-finnish"
      },
      "outputs": [],
      "source": [
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts),max_encoder_seq_length,num_encoder_tokens),dtype = 'float32')\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length , num_decoder_tokens), dtype = 'float32')\n",
        "decoder_output_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length,num_decoder_tokens) , dtype = 'float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sought-casting",
      "metadata": {
        "id": "sought-casting"
      },
      "outputs": [],
      "source": [
        "for i,(input_text,output_text) in enumerate(zip(input_texts,output_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i,t,input_token_index[char]] = 1.\n",
        "    encoder_input_data[i,t+1:,input_token_index[' ']] = 1.\n",
        "\n",
        "    for t, char in enumerate(output_text):\n",
        "        # decoder_output_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i,t,output_token_index[char]] = 1.\n",
        "        if t>0:\n",
        "            # decoder_output_data will be ahead by one timestep\n",
        "            # and will not include the start character\n",
        "            decoder_output_data[i,t-1,output_token_index[char]] = 1.\n",
        "    decoder_input_data[i,t+1:, output_token_index[' ']] = 1.\n",
        "    decoder_output_data[i , t: , output_token_index[' ']] = 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sophisticated-singapore",
      "metadata": {
        "id": "sophisticated-singapore",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3852cf9d-5627-44ad-a8c1-5546931fc8e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17, 74)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "encoder_input_data[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "backed-knife",
      "metadata": {
        "id": "backed-knife"
      },
      "outputs": [],
      "source": [
        "# define an input sequence and process it:\n",
        "encoder_inputs = Input(shape = (None, num_encoder_tokens))\n",
        "encoder = LSTM(latent_dim, return_state= True)\n",
        "encoder_outputs,state_h,state_c = encoder(encoder_inputs)\n",
        "# we discard 'encoder_outputs' and only keep the states.\n",
        "encoder_states = [state_h,state_c]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "outdoor-accident",
      "metadata": {
        "id": "outdoor-accident"
      },
      "outputs": [],
      "source": [
        "# set up the decoder , using encoder_states as initial states:\n",
        "decoder_inputs= Input(shape=(None, num_decoder_tokens))\n",
        "decoder_lstm = LSTM(latent_dim,return_sequences = True, return_state = True)\n",
        "decoder_outputs, _ ,_ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens,activation = 'softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Your existing code here...\n",
        "\n",
        "# Compile and fit the model\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_output_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_split=0.2)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict([encoder_input_data, decoder_input_data])\n",
        "predicted_labels = np.argmax(predictions, axis=-1)\n",
        "true_labels = np.argmax(decoder_output_data, axis=-1)\n",
        "\n",
        "# Flatten the arrays for confusion matrix computation\n",
        "predicted_labels_flat = predicted_labels.flatten()\n",
        "true_labels_flat = true_labels.flatten()\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(true_labels_flat, predicted_labels_flat)\n",
        "print('Confusion Matrix:')\n",
        "print(conf_matrix)\n",
        "\n",
        "# Compute accuracy, precision, recall, and F1-score\n",
        "accuracy = accuracy_score(true_labels_flat, predicted_labels_flat)\n",
        "precision = precision_score(true_labels_flat, predicted_labels_flat, average='weighted')\n",
        "recall = recall_score(true_labels_flat, predicted_labels_flat, average='weighted')\n",
        "f1 = f1_score(true_labels_flat, predicted_labels_flat, average='weighted')\n",
        "\n",
        "print('Accuracy:', accuracy)\n",
        "print('Precision:', precision)\n",
        "print('Recall:', recall)\n",
        "print('F1-score:', f1)\n"
      ],
      "metadata": {
        "id": "vM3g0e4408W5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49d1532a-3e96-4a43-86d0-ac207aaeec47"
      },
      "id": "vM3g0e4408W5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "250/250 [==============================] - 8s 17ms/step - loss: 0.1468 - accuracy: 0.9541 - val_loss: 0.5409 - val_accuracy: 0.8737\n",
            "Epoch 2/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.1448 - accuracy: 0.9551 - val_loss: 0.5434 - val_accuracy: 0.8733\n",
            "Epoch 3/100\n",
            "250/250 [==============================] - 4s 15ms/step - loss: 0.1434 - accuracy: 0.9554 - val_loss: 0.5449 - val_accuracy: 0.8742\n",
            "Epoch 4/100\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.1422 - accuracy: 0.9558 - val_loss: 0.5429 - val_accuracy: 0.8743\n",
            "Epoch 5/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.1409 - accuracy: 0.9560 - val_loss: 0.5462 - val_accuracy: 0.8737\n",
            "Epoch 6/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.1394 - accuracy: 0.9567 - val_loss: 0.5495 - val_accuracy: 0.8734\n",
            "Epoch 7/100\n",
            "250/250 [==============================] - 4s 15ms/step - loss: 0.1381 - accuracy: 0.9569 - val_loss: 0.5520 - val_accuracy: 0.8739\n",
            "Epoch 8/100\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.1367 - accuracy: 0.9574 - val_loss: 0.5547 - val_accuracy: 0.8738\n",
            "Epoch 9/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.1357 - accuracy: 0.9576 - val_loss: 0.5583 - val_accuracy: 0.8736\n",
            "Epoch 10/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.1341 - accuracy: 0.9582 - val_loss: 0.5604 - val_accuracy: 0.8731\n",
            "Epoch 11/100\n",
            "250/250 [==============================] - 4s 15ms/step - loss: 0.1333 - accuracy: 0.9583 - val_loss: 0.5620 - val_accuracy: 0.8741\n",
            "Epoch 12/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.1317 - accuracy: 0.9590 - val_loss: 0.5621 - val_accuracy: 0.8746\n",
            "Epoch 13/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.1306 - accuracy: 0.9591 - val_loss: 0.5672 - val_accuracy: 0.8738\n",
            "Epoch 14/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.1296 - accuracy: 0.9596 - val_loss: 0.5700 - val_accuracy: 0.8730\n",
            "Epoch 15/100\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.1283 - accuracy: 0.9599 - val_loss: 0.5699 - val_accuracy: 0.8736\n",
            "Epoch 16/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.1273 - accuracy: 0.9602 - val_loss: 0.5743 - val_accuracy: 0.8728\n",
            "Epoch 17/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.1260 - accuracy: 0.9606 - val_loss: 0.5787 - val_accuracy: 0.8734\n",
            "Epoch 18/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.1250 - accuracy: 0.9609 - val_loss: 0.5804 - val_accuracy: 0.8728\n",
            "Epoch 19/100\n",
            "250/250 [==============================] - 4s 14ms/step - loss: 0.1242 - accuracy: 0.9610 - val_loss: 0.5839 - val_accuracy: 0.8723\n",
            "Epoch 20/100\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.1230 - accuracy: 0.9613 - val_loss: 0.5806 - val_accuracy: 0.8734\n",
            "Epoch 21/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.1219 - accuracy: 0.9616 - val_loss: 0.5889 - val_accuracy: 0.8723\n",
            "Epoch 22/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.1205 - accuracy: 0.9621 - val_loss: 0.5895 - val_accuracy: 0.8729\n",
            "Epoch 23/100\n",
            "250/250 [==============================] - 4s 15ms/step - loss: 0.1197 - accuracy: 0.9625 - val_loss: 0.5951 - val_accuracy: 0.8731\n",
            "Epoch 24/100\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.1182 - accuracy: 0.9628 - val_loss: 0.5940 - val_accuracy: 0.8723\n",
            "Epoch 25/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.1175 - accuracy: 0.9630 - val_loss: 0.5979 - val_accuracy: 0.8722\n",
            "Epoch 26/100\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.1169 - accuracy: 0.9633 - val_loss: 0.6012 - val_accuracy: 0.8725\n",
            "Epoch 27/100\n",
            "250/250 [==============================] - 3s 14ms/step - loss: 0.1155 - accuracy: 0.9636 - val_loss: 0.5992 - val_accuracy: 0.8732\n",
            "Epoch 28/100\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.1145 - accuracy: 0.9639 - val_loss: 0.6053 - val_accuracy: 0.8723\n",
            "Epoch 29/100\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.1137 - accuracy: 0.9641 - val_loss: 0.6045 - val_accuracy: 0.8726\n",
            "Epoch 30/100\n",
            "250/250 [==============================] - 4s 14ms/step - loss: 0.1127 - accuracy: 0.9646 - val_loss: 0.6111 - val_accuracy: 0.8715\n",
            "Epoch 31/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.1117 - accuracy: 0.9647 - val_loss: 0.6073 - val_accuracy: 0.8728\n",
            "Epoch 32/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.1107 - accuracy: 0.9649 - val_loss: 0.6110 - val_accuracy: 0.8729\n",
            "Epoch 33/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.1099 - accuracy: 0.9652 - val_loss: 0.6160 - val_accuracy: 0.8724\n",
            "Epoch 34/100\n",
            "250/250 [==============================] - 4s 15ms/step - loss: 0.1087 - accuracy: 0.9656 - val_loss: 0.6210 - val_accuracy: 0.8716\n",
            "Epoch 35/100\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.1077 - accuracy: 0.9658 - val_loss: 0.6211 - val_accuracy: 0.8722\n",
            "Epoch 36/100\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.1067 - accuracy: 0.9662 - val_loss: 0.6200 - val_accuracy: 0.8726\n",
            "Epoch 37/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.1060 - accuracy: 0.9664 - val_loss: 0.6282 - val_accuracy: 0.8716\n",
            "Epoch 38/100\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.1049 - accuracy: 0.9669 - val_loss: 0.6288 - val_accuracy: 0.8713\n",
            "Epoch 39/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.1043 - accuracy: 0.9668 - val_loss: 0.6327 - val_accuracy: 0.8719\n",
            "Epoch 40/100\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.1036 - accuracy: 0.9670 - val_loss: 0.6352 - val_accuracy: 0.8718\n",
            "Epoch 41/100\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.1027 - accuracy: 0.9676 - val_loss: 0.6320 - val_accuracy: 0.8716\n",
            "Epoch 42/100\n",
            "250/250 [==============================] - 4s 14ms/step - loss: 0.1018 - accuracy: 0.9676 - val_loss: 0.6399 - val_accuracy: 0.8710\n",
            "Epoch 43/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.1010 - accuracy: 0.9678 - val_loss: 0.6395 - val_accuracy: 0.8714\n",
            "Epoch 44/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.1000 - accuracy: 0.9682 - val_loss: 0.6431 - val_accuracy: 0.8711\n",
            "Epoch 45/100\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.0989 - accuracy: 0.9685 - val_loss: 0.6444 - val_accuracy: 0.8718\n",
            "Epoch 46/100\n",
            "250/250 [==============================] - 3s 14ms/step - loss: 0.0982 - accuracy: 0.9685 - val_loss: 0.6485 - val_accuracy: 0.8712\n",
            "Epoch 47/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.0975 - accuracy: 0.9688 - val_loss: 0.6525 - val_accuracy: 0.8702\n",
            "Epoch 48/100\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.0966 - accuracy: 0.9691 - val_loss: 0.6507 - val_accuracy: 0.8713\n",
            "Epoch 49/100\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.0957 - accuracy: 0.9693 - val_loss: 0.6540 - val_accuracy: 0.8711\n",
            "Epoch 50/100\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.0952 - accuracy: 0.9696 - val_loss: 0.6517 - val_accuracy: 0.8716\n",
            "Epoch 51/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.0941 - accuracy: 0.9699 - val_loss: 0.6609 - val_accuracy: 0.8708\n",
            "Epoch 52/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.0941 - accuracy: 0.9698 - val_loss: 0.6607 - val_accuracy: 0.8713\n",
            "Epoch 53/100\n",
            "250/250 [==============================] - 3s 14ms/step - loss: 0.0927 - accuracy: 0.9703 - val_loss: 0.6626 - val_accuracy: 0.8711\n",
            "Epoch 54/100\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.0919 - accuracy: 0.9706 - val_loss: 0.6653 - val_accuracy: 0.8705\n",
            "Epoch 55/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.0915 - accuracy: 0.9707 - val_loss: 0.6721 - val_accuracy: 0.8703\n",
            "Epoch 56/100\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.0906 - accuracy: 0.9709 - val_loss: 0.6704 - val_accuracy: 0.8711\n",
            "Epoch 57/100\n",
            "250/250 [==============================] - 4s 14ms/step - loss: 0.0898 - accuracy: 0.9710 - val_loss: 0.6768 - val_accuracy: 0.8695\n",
            "Epoch 58/100\n",
            "250/250 [==============================] - 3s 14ms/step - loss: 0.0892 - accuracy: 0.9714 - val_loss: 0.6781 - val_accuracy: 0.8702\n",
            "Epoch 59/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.0883 - accuracy: 0.9715 - val_loss: 0.6776 - val_accuracy: 0.8701\n",
            "Epoch 60/100\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.0872 - accuracy: 0.9720 - val_loss: 0.6824 - val_accuracy: 0.8703\n",
            "Epoch 61/100\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.0871 - accuracy: 0.9719 - val_loss: 0.6817 - val_accuracy: 0.8702\n",
            "Epoch 62/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.0864 - accuracy: 0.9722 - val_loss: 0.6828 - val_accuracy: 0.8706\n",
            "Epoch 63/100\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.0858 - accuracy: 0.9724 - val_loss: 0.6873 - val_accuracy: 0.8702\n",
            "Epoch 64/100\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.0852 - accuracy: 0.9724 - val_loss: 0.6878 - val_accuracy: 0.8706\n",
            "Epoch 65/100\n",
            "250/250 [==============================] - 4s 15ms/step - loss: 0.0842 - accuracy: 0.9727 - val_loss: 0.6921 - val_accuracy: 0.8700\n",
            "Epoch 66/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.0839 - accuracy: 0.9728 - val_loss: 0.6912 - val_accuracy: 0.8704\n",
            "Epoch 67/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.0830 - accuracy: 0.9731 - val_loss: 0.6932 - val_accuracy: 0.8700\n",
            "Epoch 68/100\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.0824 - accuracy: 0.9732 - val_loss: 0.6966 - val_accuracy: 0.8698\n",
            "Epoch 69/100\n",
            "250/250 [==============================] - 3s 14ms/step - loss: 0.0815 - accuracy: 0.9736 - val_loss: 0.7029 - val_accuracy: 0.8696\n",
            "Epoch 70/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.0809 - accuracy: 0.9737 - val_loss: 0.7061 - val_accuracy: 0.8694\n",
            "Epoch 71/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.0800 - accuracy: 0.9740 - val_loss: 0.7062 - val_accuracy: 0.8696\n",
            "Epoch 72/100\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.0795 - accuracy: 0.9741 - val_loss: 0.7061 - val_accuracy: 0.8697\n",
            "Epoch 73/100\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.0795 - accuracy: 0.9740 - val_loss: 0.7090 - val_accuracy: 0.8695\n",
            "Epoch 74/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.0785 - accuracy: 0.9744 - val_loss: 0.7166 - val_accuracy: 0.8685\n",
            "Epoch 75/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.0779 - accuracy: 0.9746 - val_loss: 0.7155 - val_accuracy: 0.8699\n",
            "Epoch 76/100\n",
            "250/250 [==============================] - 3s 14ms/step - loss: 0.0774 - accuracy: 0.9748 - val_loss: 0.7138 - val_accuracy: 0.8695\n",
            "Epoch 77/100\n",
            "250/250 [==============================] - 3s 14ms/step - loss: 0.0770 - accuracy: 0.9747 - val_loss: 0.7170 - val_accuracy: 0.8696\n",
            "Epoch 78/100\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.0762 - accuracy: 0.9753 - val_loss: 0.7217 - val_accuracy: 0.8700\n",
            "Epoch 79/100\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.0760 - accuracy: 0.9752 - val_loss: 0.7221 - val_accuracy: 0.8688\n",
            "Epoch 80/100\n",
            "250/250 [==============================] - 4s 15ms/step - loss: 0.0748 - accuracy: 0.9756 - val_loss: 0.7241 - val_accuracy: 0.8697\n",
            "Epoch 81/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.0746 - accuracy: 0.9758 - val_loss: 0.7249 - val_accuracy: 0.8689\n",
            "Epoch 82/100\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.0740 - accuracy: 0.9758 - val_loss: 0.7262 - val_accuracy: 0.8694\n",
            "Epoch 83/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.0736 - accuracy: 0.9759 - val_loss: 0.7295 - val_accuracy: 0.8695\n",
            "Epoch 84/100\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.0733 - accuracy: 0.9759 - val_loss: 0.7301 - val_accuracy: 0.8696\n",
            "Epoch 85/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.0724 - accuracy: 0.9764 - val_loss: 0.7358 - val_accuracy: 0.8684\n",
            "Epoch 86/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.0718 - accuracy: 0.9764 - val_loss: 0.7358 - val_accuracy: 0.8691\n",
            "Epoch 87/100\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.0717 - accuracy: 0.9763 - val_loss: 0.7352 - val_accuracy: 0.8696\n",
            "Epoch 88/100\n",
            "250/250 [==============================] - 4s 15ms/step - loss: 0.0709 - accuracy: 0.9766 - val_loss: 0.7395 - val_accuracy: 0.8685\n",
            "Epoch 89/100\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.0707 - accuracy: 0.9766 - val_loss: 0.7432 - val_accuracy: 0.8689\n",
            "Epoch 90/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.0698 - accuracy: 0.9769 - val_loss: 0.7444 - val_accuracy: 0.8689\n",
            "Epoch 91/100\n",
            "250/250 [==============================] - 3s 14ms/step - loss: 0.0695 - accuracy: 0.9770 - val_loss: 0.7441 - val_accuracy: 0.8685\n",
            "Epoch 92/100\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.0689 - accuracy: 0.9773 - val_loss: 0.7489 - val_accuracy: 0.8688\n",
            "Epoch 93/100\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.0683 - accuracy: 0.9774 - val_loss: 0.7482 - val_accuracy: 0.8692\n",
            "Epoch 94/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.0680 - accuracy: 0.9774 - val_loss: 0.7513 - val_accuracy: 0.8688\n",
            "Epoch 95/100\n",
            "250/250 [==============================] - 4s 14ms/step - loss: 0.0674 - accuracy: 0.9777 - val_loss: 0.7514 - val_accuracy: 0.8690\n",
            "Epoch 96/100\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.0669 - accuracy: 0.9778 - val_loss: 0.7520 - val_accuracy: 0.8693\n",
            "Epoch 97/100\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.0667 - accuracy: 0.9778 - val_loss: 0.7550 - val_accuracy: 0.8683\n",
            "Epoch 98/100\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.0663 - accuracy: 0.9780 - val_loss: 0.7631 - val_accuracy: 0.8684\n",
            "Epoch 99/100\n",
            "250/250 [==============================] - 4s 15ms/step - loss: 0.0655 - accuracy: 0.9781 - val_loss: 0.7614 - val_accuracy: 0.8682\n",
            "Epoch 100/100\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 0.0652 - accuracy: 0.9783 - val_loss: 0.7634 - val_accuracy: 0.8687\n",
            "625/625 [==============================] - 3s 4ms/step\n",
            "Confusion Matrix:\n",
            "[[ 19995      1      0 ...      0      0      0]\n",
            " [     0 833093      0 ...      0     87      0]\n",
            " [     0      2   2251 ...      0      0      0]\n",
            " ...\n",
            " [     0      0      0 ...     32      0      0]\n",
            " [     1    109      0 ...      0    400      0]\n",
            " [     0      0      0 ...      0      0      0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9576915254237288\n",
            "Precision: 0.9575551295495514\n",
            "Recall: 0.9576915254237288\n",
            "F1-score: 0.9574536207044317\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pointed-counter",
      "metadata": {
        "id": "pointed-counter",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf6353c2-7c9b-43bc-cbed-284abb1351b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 357ms/step\n",
            "1/1 [==============================] - 0s 399ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Va aie-toi !\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Va aie-toi !\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Va aie-toi !\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Va aie-toi !\n",
            "\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Salut.\n",
            "\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Salut.\n",
            "\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Filez !\n",
            "\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Filez !\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Filez !\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Filez !\n",
            "\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Filez !\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Filez !\n",
            "\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Filez !\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Filez !\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Input sentence: Run.\n",
            "Decoded sentence: Filez !\n",
            "\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Input sentence: Run.\n",
            "Decoded sentence: Filez !\n",
            "\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Input sentence: Run.\n",
            "Decoded sentence: Filez !\n",
            "\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Input sentence: Run.\n",
            "Decoded sentence: Filez !\n",
            "\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Input sentence: Run.\n",
            "Decoded sentence: Filez !\n",
            "\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Input sentence: Run.\n",
            "Decoded sentence: Filez !\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# define sampling models:\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape = (latent_dim,))\n",
        "decoder_state_input_c = Input(shape = (latent_dim,))\n",
        "decoder_input_states = [decoder_state_input_h,decoder_state_input_c]\n",
        "\n",
        "decoder_outputs,state_h,state_c = decoder_lstm(decoder_inputs, initial_state=decoder_input_states)\n",
        "decoder_states = [state_h,state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs]+decoder_input_states,\n",
        "    [decoder_outputs] + decoder_states)\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to something readable.\n",
        "reverse_input_char_index = dict(\n",
        "    (i,char) for char,i in input_token_index.items())\n",
        "reverse_output_char_index = dict(\n",
        "    (i,char) for char,i in output_token_index.items())\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    states_value= encoder_model.predict(input_seq)\n",
        "    output_seq = np.zeros((1,1,num_decoder_tokens))\n",
        "    output_seq[0,0,output_token_index['\\t']] = 1\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentences = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens,h,c = decoder_model.predict(\n",
        "            [output_seq]+ states_value)\n",
        "\n",
        "        sampled_token_index = np.argmax(output_tokens[0,-1, :])\n",
        "        sampled_char = reverse_output_char_index[sampled_token_index]\n",
        "        decoded_sentences += sampled_char\n",
        "\n",
        "        if(sampled_char == '\\n' or len(decoded_sentences) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        #update the target sequence (of length 1):\n",
        "        output_seq = np.zeros((1,1,num_decoder_tokens))\n",
        "        output_seq[0,0,sampled_token_index] = 1\n",
        "\n",
        "        states_value = [h,c]\n",
        "    return decoded_sentences\n",
        "\n",
        "for seq_index in range(20):\n",
        "    # take one sequence for trying out decoding:\n",
        "    input_seq = encoder_input_data[seq_index:seq_index+1]\n",
        "    decoded_sentences = decode_sequence(input_seq)\n",
        "    print('Input sentence:', input_texts[seq_index])\n",
        "    print('Decoded sentence:' , decoded_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hybrid-particle",
      "metadata": {
        "id": "hybrid-particle"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}